<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NanoLLM - Rapport de Projet</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            margin: 0;
            padding: 0;
        }

        .hero {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 20px;
            text-align: center;
        }

        .hero h1 {
            font-size: 2.5rem;
            margin-bottom: 20px;
        }

        .btn-demo {
            display: inline-block;
            background: white;
            color: #764ba2;
            text-decoration: none;
            padding: 15px 30px;
            font-size: 1.1rem;
            font-weight: bold;
            border-radius: 30px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            margin-top: 20px;
        }

        .btn-demo:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.25);
        }

        .container {
            max-width: 800px;
            margin: 40px auto;
            padding: 20px;
            background: white;
            border-radius: 12px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
        }

        h2 {
            color: #764ba2;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
            margin-top: 30px;
        }
    </style>
</head>
<body>

    <div class="hero">
        <h1>Projet NanoLLM</h1>
        <p>Un modèle de langage miniature, quantifié et optimisé pour l'inférence sur CPU.</p>
        
        <a href="chat/" class="btn-demo">Try Demo</a>
    </div>

    <div class="container">
        <h2>1. Introduction</h2>
        <p>Bienvenue sur la page d'explication du projet NanoLLM. Ce document détaille l'architecture, l'entraînement et les méthodes d'optimisation utilisées (comme Transformer Engine et la quantification torchao).</p>

        <h2>2. Architecture (LLaMA-like)</h2>
        <p>Le modèle est architecturé autour d'un Transformer classique avec les améliorations suivantes :</p>
        <ul>
            <li><strong>RMSNorm :</strong> Remplacement de la LayerNorm standard pour une meilleure stabilité numérique en grande dimension.</li>
            <li><strong>SwiGLU :</strong> Fonction d'activation optimisée.</li>
            <li><strong>RoPE :</strong> Rotary Positional Embeddings.</li>
        </ul>

        <h2>3. Optimisation et Inférence</h2>
        <p>Pour héberger ce modèle sur un serveur gratuit (CPU Dual-Core), une quantification <strong>INT8 (Weight-Only)</strong> a été implémentée, divisant la bande passante mémoire nécessaire par deux tout en préservant la qualité de génération.</p>
        
        </div>

</body>
</html>