<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NanoLLM - Project Report</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
            margin: 0;
            padding: 0;
        }

        .hero {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 20px;
            text-align: center;
        }

        .hero h1 {
            font-size: 2.5rem;
            margin-bottom: 20px;
        }

        .btn-demo {
            display: inline-block;
            background: white;
            color: #764ba2;
            text-decoration: none;
            padding: 15px 30px;
            font-size: 1.1rem;
            font-weight: bold;
            border-radius: 30px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            transition: transform 0.2s, box-shadow 0.2s;
            margin-top: 20px;
        }

        .btn-demo:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(0,0,0,0.25);
        }

        .container {
            max-width: 800px;
            margin: 40px auto;
            padding: 20px;
            background: white;
            border-radius: 12px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
        }

        h2 {
            color: #764ba2;
            border-bottom: 2px solid #eee;
            padding-bottom: 10px;
            margin-top: 30px;
        }
    </style>
</head>
<body>

    <div class="hero">
        <h1>NanoLLM Project</h1>
        <p>A miniature language model.</p>
        
        <a href="chat/" class="btn-demo">Try Demo</a>
    </div>

    <div class="container">
        <h2>1. Introduction</h2>
        <p>Welcome to the NanoLLM project explanation page. This document details the architecture, training, and optimization methods.</p>

        <h2>2. Architecture (LLaMA-like)</h2>
        <p>The model is architected around a classic Transformer with the following improvements:</p>
        <ul>
            <li><strong>SwiGLU:</strong> Optimized activation function.</li>
            <li><strong>RoPE:</strong> Rotary Positional Embeddings.</li>
        </ul>

        <h2>3. Optimization and Inference</h2>
        <p>To host this model on a free server (Dual-Core CPU)</p>
        
        </div>

</body>
</html>